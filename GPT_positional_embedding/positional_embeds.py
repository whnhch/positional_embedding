# -*- coding: utf-8 -*-
"""Positional_Encoding_At_Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wDeaZFGaKu9i6Fy0rmJwvesdX3sg3huM

# **Implement Positional Encoding with GPT2 output**
----
"""

# #### **Install trsansformers** 

# package for using BERT from huggingface
# https://huggingface.co/transformers/index.html

# !pip install transformers

# for gpu
import torch
import math
import os

if torch.cuda.is_available():
    device = torch.device('cuda')
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))

else:
    print('No GPU avbailable, using the CPU instead')
    device = torch.device('cpu')

import os

os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

"""## **1. Implement GPT2 Decoder**"""

import transformers
import torch
from transformers import BertTokenizer
from PIL import Image
import numpy as np

"""### **GPT2 tokenizer utils**

"""

print('Loading BERT tokenizer...')
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
# gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')

# gpt_tokenizer.pad_token = gpt_tokenizer.eos_token

object_type_list = ['OBJORGANIZATION',
                    'OBJNATIONALITY',
                    'OBJMISC',
                    'OBJRELIGION',
                    'OBJNUMBER',
                    'OBJCRIMINALCHARGE',
                    'OBJIDEOLOGY',
                    'OBJSTATEORPROVINCE',
                    'OBJCITY',
                    'OBJCOUNTRY',
                    'OBJPERSON',
                    'OBJTITLE',
                    'OBJLOCATION',
                    'OBJCAUSEOFDEATH',
                    'OBJDURATION',
                    'OBJURL',
                    'OBJDATE']
subject_type_list = ['SUBJORGANIZATION', 'SUBJPERSON']

type_list = object_type_list + subject_type_list
bert_tokenizer.add_tokens(type_list, special_tokens=True)


def getSubjStart(sentence, s):
    if sentence[s] in subject_type_list:
        return s
    return -1


def getSubjEnd(sentence, s):
    se = s
    if sentence[s] in subject_type_list:
        se += 1
    return se


def getObjStart(sentence, o):
    if sentence[o] in object_type_list:
        return o
    return -1


def getObjEnd(sentence, o):
    oe = o
    if sentence[o] in object_type_list:
        oe += 1
    return oe


def isEnd(i, length):
    if i + 1 >= length:
        return True
    return False


def getPE(max_length, sentence):
    length = len(sentence)
    ps = [0] * (length)
    po = [0] * (length)

    ss, se, os, oe = 0, 0, 0, 0

    flagS = False
    flagO = False
    for i in range(length):
        if sentence[i] == 'PAD':
            break

        if not isEnd(i, length):
            tmps = getSubjStart(sentence, i)
            tmpo = getObjStart(sentence, i)

            if tmps != -1 and not flagS:
                ss = tmps
                flagS = True
            elif tmps != -1 and flagS:
                tmp = getSubjEnd(sentence, i)
                if tmp > ss:
                    se = tmp

            if tmpo != -1 and not flagO:
                os = tmpo
                flagO = True
            elif tmpo != -1 and flagO:
                tmp = getObjEnd(sentence, i)
                if tmp > os:
                    oe = tmp

    real_length = 0
    firstSEP = True
    for i in range(len(sentence)):
        if i != 0 and real_length == i:
            break
        if sentence[i] == '[SEP]' and firstSEP:
            firstSEP = False
            real_length = i
            break
        elif sentence[i] == '[SEP]' and not firstSEP:
            break

        elif sentence[i] == '[PAD]' or sentence[i] == '[UKN]':
            ps[i] = 0
            po[i] = 0
            continue

        # -127 ... 0 ... 127
        # 1 ...   128 ... 256
        if i < ss:
            ps[i] = abs(i - ss)
        elif ss <= i <= se:
            ps[i] = max_length
        elif i > se:
            ps[i] = (i - se) + max_length

        if i < os:
            po[i] = abs(i - os)
        elif os <= i <= oe:
            po[i] = max_length
        elif i > oe:
            po[i] = (i - oe) + max_length

        # if real_length == 0:
        #   real_length = max_length
    return [ps, po], real_length


def get_bilstm_positional_ids(max_length, word_sequences):
    return getPE(max_length, word_sequences)


def get_bert_token_probs(bert_input_ids, words, word_probs, max_len):
    tokens = bert_tokenizer.convert_ids_to_tokens(bert_input_ids)
    output = [torch.zeros(max_len).to('cpu')] * len(tokens)
    i = 0
    k = i
    while i < len(words):

        token = tokens[k]
        if token == '[CLS]':
            k += 1
            continue
        if token == '[PAD]' or token == '[SEP]':
            break

        if not '##' in token:
            tmp_tokens = []
            tmp_tokens.append(token)

            j = k + 1
            while j < len(tokens):
                # print('tmp_tokens ', tmp_tokens)
                token = tokens[j]
                if not '##' in token:
                    j -= 1
                    break
                else:
                    tmp_tokens.append(token.replace('#', ''))
                    j += 1

            word = ''.join(tmp_tokens)
            if words[i] == word:
                for w in range(k, j + 1):
                    output[w] = word_probs[i].to(
                        'cpu')  # due to [cls] first token in bert words vs first token in gpt words
            else:
                try:
                    idx = type_list.index(word)
                    for w in range(k, j + 1):
                        output[w] = word_probs[i].to('cpu')
                except:
                    cur_word = words[i]
                    # print('cur word', cur_word)
                    bert_tokens = bert_tokenizer.convert_ids_to_tokens(
                        bert_tokenizer(cur_word, add_special_tokens=False)['input_ids'])
                    # print('cur bert tokens ', bert_tokens)
                    token_len = len(bert_tokens)
                    # print('cur s ', tokens[k:k+token_len])
                    if tokens[k:k + token_len] == bert_tokens:
                        for w in range(k, k + token_len):
                            output[w] = word_probs[i].to('cpu')
                        j = k + token_len - 1
                    else:
                        print('Error at get bert token probabilities! : No matching word in bert tokens. idx ', i,
                              ' word ', word, ' gpt ', bert_tokens)
                        token_len = len(bert_tokens)
                        for w in range(k, k + token_len):
                            output[w] = word_probs[i].to('cpu')
                        j = k + token_len - 1

            k = j + 1
            i += 1

        else:
            output[k] = word_probs[i].to('cpu')
            k += 1
            i += 1

    return torch.stack(output).to('cpu')


def get_bert_tokens(tokenizer, sentences, labels, max_len):
    input_ids = []
    attention_masks = []
    positional_ids = []
    sequence_lengths = []
    # pos_probs = []

    for i in range(len(sentences)):
        sent = sentences[i]
        encoded_dict = tokenizer.encode_plus(
            sent,  # Sentence to encode.
            add_special_tokens=False,  # Add '[CLS]' and '[SEP]'
            max_length=max_len,  # Pad & truncate all sentences.
            pad_to_max_length=True,
            return_attention_mask=True,  # Construct attn. masks.
            return_tensors='pt',  # Return pytorch tensors.
        )
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])

        tokens = tokenizer.convert_ids_to_tokens(encoded_dict['input_ids'][0, :])

        # pos_probs.append(get_bert_token_probs(tokens, words[i], words_probs[i],max_len))

        positional_id, sequence_length = get_bilstm_positional_ids(max_len, tokens)
        positional_ids.append(positional_id)
        sequence_lengths.append(sequence_length)

        # if i == 2:
        # break

    # must drop out after SEP of object and subject
    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    labels = torch.tensor(labels)

    positional_ids = torch.tensor(positional_ids)
    sequence_lengths = torch.tensor(sequence_lengths)
    # pos_probs = torch.stack(pos_probs)

    return input_ids, attention_masks, positional_ids, sequence_lengths, labels


"""### **Positional Encoding with GPT2 output** """


def get_one_hot_encoding(max_len, dim=768):
    pe = torch.zeros(max_len, dim)

    for i in range(max_len):
        pe[i, i] = 1

    return pe


def get_linear_embedding(max_len, dim=768):
    isOdd = False

    if dim % 2 != 0:
        isOdd = True

    pe = torch.zeros(max_len, dim)

    width_ratio = 1 / (int(dim / 2))
    height_ratio = 2 / (max_len - 1)

    for i in range(dim):
        pe[0, i] = 1 - (2 / (dim - 1)) * i

    for i in range(1, max_len):
        for j in range(int(dim / 2)):
            pe[i, j] = pe[i - 1, j] - (height_ratio - (height_ratio * width_ratio * j))
            pe[i, dim - 1 - j] = pe[i - 1, dim - 1 - j] + (height_ratio - (height_ratio * width_ratio * j))
        if isOdd:
            pe[i, int(dim / 2) + 1] = pe[i - 1, int(dim / 2) + 1] - (height_ratio - (height_ratio * width_ratio * j))

    return pe


def get_sinusoidal_embedding(max_len, dim=768):
    pe = torch.zeros(max_len, dim)

    for pos in range(max_len):
        for i in range(0, dim, 2):
            pe[pos, i] = \
                math.sin(pos / (10000 ** ((2 * i) / dim)))
            pe[pos, i + 1] = \
                math.cos(pos / (10000 ** ((2 * (i + 1)) / dim)))

    return pe


import pandas as pd
import numpy as np

label_list = ['org:member_of', 'per:schools_attended', 'per:charges', 'org:city_of_headquarters',
              'org:country_of_headquarters', 'org:subsidiaries', 'per:employee_of', 'per:stateorprovince_of_death',
              'per:stateorprovince_of_birth', 'per:country_of_death', 'org:shareholders', 'per:countries_of_residence',
              'per:children', 'org:alternate_names', 'per:alternate_names', 'per:stateorprovinces_of_residence',
              'per:country_of_birth', 'org:founded_by', 'org:parents', 'org:stateorprovince_of_headquarters',
              'org:dissolved', 'org:members', 'per:age', 'per:spouse', 'org:website', 'per:cities_of_residence',
              'per:parents', 'per:cause_of_death', 'per:date_of_death', 'per:origin', 'no_relation', 'per:religion',
              'org:political/religious_affiliation', 'per:siblings', 'org:founded', 'per:date_of_birth',
              'per:city_of_death', 'org:number_of_employees/members', 'org:top_members/employees', 'per:other_family',
              'per:title', 'per:city_of_birth'
              ]

train_df = pd.read_csv('./dataset/train_masked.tsv', delimiter='\t', header=None,
                       names=['sentence_id', 'label', 'label_notes', 'sentence', 'masked_sentence'])
dev_df = pd.read_csv('./dataset/dev_masked.tsv', delimiter='\t', header=None,
                     names=['sentence_id', 'label', 'label_notes', 'sentence', 'masked_sentence'])
test_df = pd.read_csv('./dataset/test_masked.tsv', delimiter='\t', header=None,
                      names=['sentence_id', 'label', 'label_notes', 'sentence', 'masked_sentence'])

train_sentences = np.array(train_df.sentence.values)
train_masked_sentences = np.array(train_df.masked_sentence.values)
train_labels = np.array([label_list.index(label) for label in train_df.label.values])

dev_sentences = np.array(dev_df.sentence.values)
dev_masked_sentences = np.array(dev_df.masked_sentence.values)
dev_labels = np.array([label_list.index(label) for label in dev_df.label.values])

test_sentences = np.array(test_df.sentence.values)
test_masked_sentences = np.array(test_df.masked_sentence.values)
test_labels = np.array([label_list.index(label) for label in test_df.label.values])
"""### **Make torch dataloader**

Minimum of train sequence length should not be zero it means that there is any sequeence which size is over MAX_LEN
"""

print('open bert data...')

tr_input_ids = torch.load('./dataset/tr_input_ids.pt').to('cpu')
tr_attribute_masks = torch.load('./dataset/tr_attribute_masks.pt').to('cpu')
tr_positional_ids = torch.load('./dataset/tr_positional_ids.pt').to('cpu')
tr_seq_length = torch.load('./dataset/tr_seq_length.pt').to('cpu')
tr_gt_labels = torch.load('./dataset/tr_gt_labels.pt').to('cpu')

print(tr_input_ids.device)
#
# dv_input_ids = torch.load('./dataset/dv_input_ids.pt').to('cpu')
# dv_attribute_masks = torch.load('./dataset/dv_attribute_masks.pt').to('cpu')
# dv_positional_ids = torch.load('./dataset/dv_positional_ids.pt').to('cpu')
# dv_seq_length = torch.load('./dataset/dv_seq_length.pt').to('cpu')
# dv_gt_labels = torch.load('./dataset/dv_gt_labels.pt').to('cpu')

batch_size = 3
max_len = 450

import pickle

print('open gpt words and probs...')

test_words = None
test_probs = None
dev_words = None
dev_probs = None
train_words = None
train_probs = None

# with open('dev_words.txt', 'rb') as f1, open('dev_probs.txt', 'rb') as f2:
#     dev_words = pickle.load(f1)
#     dev_probs = pickle.load(f2)

import torch

with open('tr_words.txt', 'rb') as f1, open('tr_probs.txt', 'rb') as f2:
    train_words = pickle.load(f1)
    train_probs = pickle.load(f2)

# with open('test_words.txt', 'rb') as f1, open('test_probs.txt', 'rb') as f2:
#     test_words = pickle.load(f1)
#     test_probs = pickle.load(f2)
#

def get_prob_embeds(bert_input_ids, words, probs, max_len, dim=768):
    bert_token_probs = get_bert_token_probs(bert_input_ids, words, probs, max_len).to('cpu')
    # .to(device)

    # print('bert token probs shape ',bert_token_probs.shape)
    embeds = get_linear_embedding(max_len).to('cpu')
    # .to(device)

    for i in range(len(bert_token_probs)):
        for j in range(i + 1, max_len - 1):
            # skip [cls]
            width_ratio = 1 / (int(768 / 2))
            height_ratio = 2 / (max_len - 1)

            difference = (height_ratio - (height_ratio * width_ratio * j))
            cur_probs = bert_token_probs[i][j]  # without cls

            if cur_probs >= 0.5:
                embeds[j + 1, :int(dim / 2)] = embeds[j + 1, :int(dim / 2)] + difference * cur_probs
                embeds[j + 1, int(dim / 2):] = embeds[j + 1, int(dim / 2):] - difference * cur_probs

            else:
                embeds[j + 1, :int(dim / 2)] = embeds[j + 1, :int(dim / 2)] - difference * (1 - cur_probs)
                embeds[j + 1, int(dim / 2):] = embeds[j + 1, int(dim / 2):] + difference * (1 - cur_probs)

    return embeds


import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

batch_size = 1
max_len = 450
dim = 768

print('make train position embedding...')

len(tr_input_ids)


def save_embeds(input_ids, words, probs, type='train', start_idx=0, max_len=450, dim=768):
    prob_embeds = torch.zeros((32, max_len, dim))

    for idx in range(start_idx, len(input_ids)):
        if idx % 32 == 0 and idx != 0 and idx != start_idx:
            filename = type+'_prob_embeds_' + str(idx) + '_.pt'
            torch.save(prob_embeds, filename)
            prob_embeds = torch.zeros((32, max_len, dim))

        prob_embeds[idx % 32, :] = get_prob_embeds(input_ids[idx], words[idx], probs[idx], max_len)


try:
    save_embeds(tr_input_ids, train_words, train_probs, type='train', start_idx=34144)
    # save_embeds(dv_input_ids, dev_words, dev_probs, type='dev')
except:
        os.system('shutdown /r /t 1')

os.system('shutdown /r /t 1')

# save_embeds(test_input_ids, test_words, test_probs)

# print(torch.min(tr_seq_length)) # should not be zero it means that there is any sequeence which size is over MAX_LEN

# len(dv_input_ids)

# B batch size, deafult = 32
# P length of sequence, default = 450
# H hidden size of BERT, default = 768
# E embedding dimension of embedding layer, default = 20
