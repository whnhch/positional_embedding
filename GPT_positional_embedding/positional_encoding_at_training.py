# -*- coding: utf-8 -*-
"""Positional_Encoding_At_Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wDeaZFGaKu9i6Fy0rmJwvesdX3sg3huM

# **Implement Positional Encoding with GPT2 output**
----
"""

# #### **Install trsansformers** 

# package for using BERT from huggingface
# https://huggingface.co/transformers/index.html

# !pip install transformers

# for gpu
import torch
import math
import sys

if torch.cuda.is_available():
    device = torch.device('cuda')
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))

else:
    print('No GPU avbailable, using the CPU instead')
    device = torch.device('cpu')

import os

os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

"""## **1. Implement GPT2 Decoder**"""

import transformers
import torch
from transformers import GPT2Model
from PIL import Image
import numpy as np


class GPTDecoder(torch.nn.Module):
    def __init__(self, pretrained_model_name, max_len):
        super().__init__()

        self.decoder = GPT2Model.from_pretrained(pretrained_model_name)
        self.wte = self.decoder.get_input_embeddings().weight
        self.softmax = torch.nn.Softmax(dim=2)
        self.maxlen = max_len

    def get_decoder_output(self, inputs):
        outputs = self.decoder(inputs)
        last_hidden_state = outputs.last_hidden_state
        return last_hidden_state

    def get_logits(self, last_hidden_state):
        return torch.matmul(last_hidden_state, self.wte.T)  # (B,P,768)

    def get_probs(self, logits):
        return self.softmax(logits)

    def get_token_probs(self, input_ids, probs, attention_masks):
        output_prob = []
        # probs only of sequences
        # batch가 없다면
        for b in range(len(input_ids)):
            mask_idx = (attention_masks[b] == 0).nonzero()[0][0]

            seq_prob = []
            for i in range(self.maxlen):
                tok_prob = probs[b, i, input_ids[b, :]]  # (1, H)
                tok_prob[mask_idx:] = 0
                seq_prob.append(tok_prob)  # (i, H)
            output_prob.append(seq_prob)  # ( b, P', P')

        return output_prob

    def forward(self, input_ids, attention_masks):  # (B, P)
        last_hidden_state = self.get_decoder_output(input_ids)  # (B, P', H)
        logits = self.get_logits(last_hidden_state)  # (B, P', len(vocab))
        probs = self.get_probs(logits)  # (B, P', len(vocab))

        output_prob = self.get_token_probs(input_ids, probs, attention_masks)  # (B, P', P')

        return output_prob

from transformers import BertTokenizer, GPT2Tokenizer

print('Loading BERT tokenizer...')
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

object_type_list = ['OBJORGANIZATION',
                    'OBJNATIONALITY',
                    'OBJMISC',
                    'OBJRELIGION',
                    'OBJNUMBER',
                    'OBJCRIMINALCHARGE',
                    'OBJIDEOLOGY',
                    'OBJSTATEORPROVINCE',
                    'OBJCITY',
                    'OBJCOUNTRY',
                    'OBJPERSON',
                    'OBJTITLE',
                    'OBJLOCATION',
                    'OBJCAUSEOFDEATH',
                    'OBJDURATION',
                    'OBJURL',
                    'OBJDATE']
subject_type_list = ['SUBJORGANIZATION', 'SUBJPERSON']

type_list = object_type_list + subject_type_list
bert_tokenizer.add_tokens(type_list, special_tokens=True)

def get_prob_embeds(b_probs, max_len, dim=768):
    embeds = []
    for b in range(len(b_probs)):
        embed = get_linear_embedding(max_len)
        probs = b_probs[b]

        for i in range(len(probs)):
            for j in range(i + 1, max_len - 1):
                width_ratio = 1 / (int(768 / 2))
                height_ratio = 2 / (max_len - 1)

                difference = (height_ratio - (height_ratio * width_ratio * j))
                cur_probs = probs[i][j]  # without cls

                if cur_probs >= 0.5:
                    embed[j + 1, :int(dim / 2)] = embed[j + 1, :int(dim / 2)] + difference * cur_probs
                    embed[j + 1, int(dim / 2):] = embed[j + 1, int(dim / 2):] - difference * cur_probs

                else:
                    embed[j + 1, :int(dim / 2)] = embed[j + 1, :int(dim / 2)] - difference * (1 - cur_probs)
                    embed[j + 1, int(dim / 2):] = embed[j + 1, int(dim / 2):] + difference * (1 - cur_probs)

        embeds.append(embed)

    return torch.Tensor(embeds)


"""### **Positional Encoding with GPT2 output** """


def get_one_hot_encoding(max_len, dim=768):
    pe = torch.zeros(max_len, dim)

    for i in range(max_len):
        pe[i, i] = 1

    return pe


def get_linear_embedding(max_len, dim=768):
    isOdd = False

    if dim % 2 != 0:
        isOdd = True

    pe = torch.zeros(max_len, dim)

    width_ratio = 1 / (int(dim / 2))
    height_ratio = 2 / (max_len - 1)

    for i in range(dim):
        pe[0, i] = 1 - (2 / (dim - 1)) * i

    for i in range(1, max_len):
        for j in range(int(dim / 2)):
            pe[i, j] = pe[i - 1, j] - (height_ratio - (height_ratio * width_ratio * j))
            pe[i, dim - 1 - j] = pe[i - 1, dim - 1 - j] + (height_ratio - (height_ratio * width_ratio * j))
        if isOdd:
            pe[i, int(dim / 2) + 1] = pe[i - 1, int(dim / 2) + 1] - (height_ratio - (height_ratio * width_ratio * j))

    return pe


def get_sinusoidal_embedding(max_len, dim=768):
    pe = torch.zeros(max_len, dim)

    for pos in range(max_len):
        for i in range(0, dim, 2):
            pe[pos, i] = \
                math.sin(pos / (10000 ** ((2 * i) / dim)))
            pe[pos, i + 1] = \
                math.cos(pos / (10000 ** ((2 * (i + 1)) / dim)))

    return pe


import pandas as pd
import numpy as np

label_list = ['org:member_of', 'per:schools_attended', 'per:charges', 'org:city_of_headquarters',
              'org:country_of_headquarters', 'org:subsidiaries', 'per:employee_of', 'per:stateorprovince_of_death',
              'per:stateorprovince_of_birth', 'per:country_of_death', 'org:shareholders', 'per:countries_of_residence',
              'per:children', 'org:alternate_names', 'per:alternate_names', 'per:stateorprovinces_of_residence',
              'per:country_of_birth', 'org:founded_by', 'org:parents', 'org:stateorprovince_of_headquarters',
              'org:dissolved', 'org:members', 'per:age', 'per:spouse', 'org:website', 'per:cities_of_residence',
              'per:parents', 'per:cause_of_death', 'per:date_of_death', 'per:origin', 'no_relation', 'per:religion',
              'org:political/religious_affiliation', 'per:siblings', 'org:founded', 'per:date_of_birth',
              'per:city_of_death', 'org:number_of_employees/members', 'org:top_members/employees', 'per:other_family',
              'per:title', 'per:city_of_birth'
              ]

train_df = pd.read_csv('./dataset/train_masked.tsv', delimiter='\t', header=None,
                       names=['sentence_id', 'label', 'label_notes', 'sentence', 'masked_sentence'])
dev_df = pd.read_csv('./dataset/dev_masked.tsv', delimiter='\t', header=None,
                     names=['sentence_id', 'label', 'label_notes', 'sentence', 'masked_sentence'])
test_df = pd.read_csv('./dataset/test_masked.tsv', delimiter='\t', header=None,
                      names=['sentence_id', 'label', 'label_notes', 'sentence', 'masked_sentence'])

"""### **Make torch dataloader**

Minimum of train sequence length should not be zero it means that there is any sequeence which size is over MAX_LEN
"""

print('open bert data...')

tr_input_ids = torch.load('./dataset/tr_input_ids.pt').to('cpu')
tr_attribute_masks = torch.load('./dataset/tr_attribute_masks.pt').to('cpu')
tr_positional_ids = torch.load('./dataset/tr_positional_ids.pt').to('cpu')
tr_seq_length = torch.load('./dataset/tr_seq_length.pt').to('cpu')
tr_gt_labels = torch.load('./dataset/tr_gt_labels.pt').to('cpu')

print(tr_input_ids.device)
#
dv_input_ids = torch.load('./dataset/dv_input_ids.pt').to('cpu')
dv_attribute_masks = torch.load('./dataset/dv_attribute_masks.pt').to('cpu')
dv_positional_ids = torch.load('./dataset/dv_positional_ids.pt').to('cpu')
dv_seq_length = torch.load('./dataset/dv_seq_length.pt').to('cpu')
dv_gt_labels = torch.load('./dataset/dv_gt_labels.pt').to('cpu')

batch_size = 3
max_len = 450

import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

batch_size = 3
max_len = 450
dim = 768

print('make train dataset...')

train_dataset = TensorDataset(tr_input_ids, tr_attribute_masks, tr_positional_ids, tr_seq_length, tr_gt_labels)
train_sampler = RandomSampler(train_dataset)
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)
#

len(dv_input_ids)
dev_prob_embeds = torch.zeros((len(dv_input_ids), max_len, dim))

dev_dataset = TensorDataset(dv_input_ids, dv_attribute_masks, dv_positional_ids, dv_seq_length, dv_gt_labels)
dev_sampler = SequentialSampler(dev_dataset)
dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=batch_size)

# B batch size, deafult = 32
# P length of sequence, default = 450
# H hidden size of BERT, default = 768
# E embedding dimension of embedding layer, default = 20

from transformers import BertModel, AdamW, BertConfig
import torch.nn as nn
from torch.autograd import Variable
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence


class BERT_BiLSTM(nn.Module):
    def __init__(self, tokenizer, max_length=450, num_labels=42, embedding_dim=20, num_recurrent_layers=1,
                 bidirectional=True, lstm_hidden_size=768, mlp_hidden_size=300) -> None:
        # super(BERT_BiLSTM, self).__init__()
        # Feed at [CLS] token

        super().__init__()
        self.tokenizer = tokenizer
        self.config = BertConfig.from_pretrained('bert-base-cased')
        self.config.output_hidden_states = True
        self.config.output_attentions = True

        self.bert = BertModel.from_pretrained("bert-base-cased",
                                              config=self.config
                                              # , add_pooling_layer=False
                                              )
        self.bert.resize_token_embeddings(len(tokenizer))
        self.dropout = nn.Dropout(0.1)
        self.num_labels = num_labels
        self.embedding_dim = embedding_dim
        self.embedding_layer = nn.Embedding(num_embeddings=max_length * 2,
                                            embedding_dim=embedding_dim,
                                            padding_idx=0
                                            )

        self.num_recurrent_layers = num_recurrent_layers
        self.bidirectional = bidirectional
        self.lstm_hidden_size = lstm_hidden_size

        self.lstm = nn.LSTM(self.lstm_hidden_size + self.embedding_dim + self.embedding_dim,
                            hidden_size=self.lstm_hidden_size,
                            batch_first=True,
                            bidirectional=True
                            )

        self.mlp = nn.Linear(in_features=self.lstm_hidden_size * 2, out_features=mlp_hidden_size)
        self.classifier = nn.Linear(in_features=mlp_hidden_size, out_features=self.num_labels)

    def forward(self, input_ids, attention_mask, positional_ids, seq_lengths, sequence_length, labels=None,
                position_embeds=None):
        # outputs: (last_encoder_layer, pooled_output, attention_weight)
        # Set add_pooling_layer
        bert_outputs = self.bert(input_ids=input_ids,
                                 attention_mask=attention_mask,
                                 encoder_hidden_states=torch.FloatTensor(
                                     (input_ids.shape[0], sequence_length, sequence_length)),
                                 position_embeds=position_embeds
                                 )

        sequence_output = bert_outputs[0]  # (B, P, H)

        for i in range(len(input_ids)):
            sequence_output[i, seq_lengths[i]:] = 0

        # LSTM
        ps = positional_ids[:, 0, :]
        po = positional_ids[:, 1, :]

        ps = self.embedding_layer(ps)
        po = self.embedding_layer(po)

        lstm_input = torch.cat((sequence_output, ps, po), dim=-1)  # (B, P, H+E+E)

        packed_lstm_input = pack_padded_sequence(lstm_input, seq_lengths.tolist(), batch_first=True,
                                                 enforce_sorted=False)

        h0 = torch.zeros(self.num_recurrent_layers * 2, input_ids.shape[0], self.lstm_hidden_size).to(device)
        c0 = torch.zeros(self.num_recurrent_layers * 2, input_ids.shape[0], self.lstm_hidden_size).to(device)

        packed_lstm_output, (lstm_hidden, lstm_cell) = self.lstm(packed_lstm_input, (h0, c0))  # (B, P, H*2) ...

        # mlp
        mlp_input = torch.cat((lstm_hidden[-2, :, :], lstm_hidden[-1, :, :]), dim=1)
        mlp_output = self.mlp(mlp_input)  # (B, H*2)

        # last layer
        # calcuate logits
        logits = self.classifier(mlp_output)

        # calcuate loss
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

        return logits, loss


"""### **Generate model**"""

max_len = 450
model = BERT_BiLSTM(bert_tokenizer, max_length=max_len)
model.to(device)

batch_size = 3  # (set when creating our DataLoaders)
learning_rate = 1e-5
epochs = 3

optimizer = AdamW(model.parameters(),
                  lr=learning_rate,  # args.learning_rate - default is 5e-5, our notebook had 2e-5
                  eps=1e-12  # args.adam_epsilon  - default is 1e-8.
                  )

from transformers import get_linear_schedule_with_warmup

total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=0,  # Default value in run_glue.py
                                            num_training_steps=total_steps)

import numpy as np
import torch.nn.functional as F


# Function to calculate the accuracy of our predictions vs labels
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)


import time
import datetime


def format_time(elapsed):
    '''
    Takes a time in seconds and returns a string hh:mm:ss
    '''
    # Round to the nearest second.
    elapsed_rounded = int(round((elapsed)))

    # Format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))


from collections import Counter

NO_RELATION = "no_relation"


def score(key, prediction, verbose=False):
    correct_by_relation = Counter()
    guessed_by_relation = Counter()
    gold_by_relation = Counter()

    # Loop over the data to compute a score
    for row in range(len(key)):
        gold = key[row]
        guess = prediction[row]

        if gold == NO_RELATION and guess == NO_RELATION:
            pass
        elif gold == NO_RELATION and guess != NO_RELATION:
            guessed_by_relation[guess] += 1
        elif gold != NO_RELATION and guess == NO_RELATION:
            gold_by_relation[gold] += 1
        elif gold != NO_RELATION and guess != NO_RELATION:
            guessed_by_relation[guess] += 1
            gold_by_relation[gold] += 1
            if gold == guess:
                correct_by_relation[guess] += 1

    # Print verbose information
    if verbose:
        print("Per-relation statistics:")
        relations = gold_by_relation.keys()
        longest_relation = 0
        for relation in sorted(relations):
            longest_relation = max(len(relation), longest_relation)
        for relation in sorted(relations):
            # (compute the score)
            correct = correct_by_relation[relation]
            guessed = guessed_by_relation[relation]
            gold = gold_by_relation[relation]
            prec = 1.0
            if guessed > 0:
                prec = float(correct) / float(guessed)
            recall = 0.0
            if gold > 0:
                recall = float(correct) / float(gold)
            f1 = 0.0
            if prec + recall > 0:
                f1 = 2.0 * prec * recall / (prec + recall)
            # (print the score)
            sys.stdout.write(("{:<" + str(longest_relation) + "}").format(relation))
            sys.stdout.write("  P: ")
            if prec < 0.1: sys.stdout.write(' ')
            if prec < 1.0: sys.stdout.write(' ')
            sys.stdout.write("{:.2%}".format(prec))
            sys.stdout.write("  R: ")
            if recall < 0.1: sys.stdout.write(' ')
            if recall < 1.0: sys.stdout.write(' ')
            sys.stdout.write("{:.2%}".format(recall))
            sys.stdout.write("  F1: ")
            if f1 < 0.1: sys.stdout.write(' ')
            if f1 < 1.0: sys.stdout.write(' ')
            sys.stdout.write("{:.2%}".format(f1))
            sys.stdout.write("  #: %d" % gold)
            sys.stdout.write("\n")
        print("")

    # Print the aggregate score
    if verbose:
        print("Final Score:")
    prec_micro = 1.0
    if sum(guessed_by_relation.values()) > 0:
        prec_micro = float(sum(correct_by_relation.values())) / float(sum(guessed_by_relation.values()))
    recall_micro = 0.0
    if sum(gold_by_relation.values()) > 0:
        recall_micro = float(sum(correct_by_relation.values())) / float(sum(gold_by_relation.values()))
    f1_micro = 0.0
    if prec_micro + recall_micro > 0.0:
        f1_micro = 2.0 * prec_micro * recall_micro / (prec_micro + recall_micro)
    print("Precision (micro): {:.3%}".format(prec_micro))
    print("   Recall (micro): {:.3%}".format(recall_micro))
    print("       F1 (micro): {:.3%}".format(f1_micro))
    return prec_micro, recall_micro, f1_micro


import random
import numpy as np
import torch.nn.functional as F

seed_val = 42
dim = 768
max_len = 450

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

training_stats = []

decoder = GPTDecoder('gpt2-medium', max_len)
decoder.to(device)
decoder.eval()

total_t0 = time.time()

MAX_LEN = 450

# For each epoch...
for epoch_i in range(epochs):

    # ========================================
    #               Training
    # ========================================

    # Perform one full pass over the training set.

    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    print('Training...')

    # Measure how long the training epoch takes.
    t0 = time.time()

    # Reset the total loss for this epoch.
    total_train_loss = []
    total_train_accuracy = []
    predictions_, true_labels_ = [], []

    model.train()

    # For each batch of training data...
    for step, batch in enumerate(train_dataloader):

        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_input_positional_ids = batch[2].to(device)
        b_input_seq_lengths = batch[3].to(device)
        b_labels = batch[4].to(device)

        with torch.no_grad():
            probs = decoder(b_input_ids, b_input_mask)
        b_prob_embeds = get_prob_embeds(probs, max_len)

        model.zero_grad()

        cur_len = len(b_input_ids)

        b_input_seq_lengths, indicies = torch.sort(b_input_seq_lengths, dim=0, descending=True)

        def smart_sort(x, per):
            z = torch.empty_like(x)
            for i in range(len(per)):
                z[per[i]] = x[i]
            return z


        b_input_ids = smart_sort(b_input_ids, indicies)
        b_input_mask = smart_sort(b_input_mask, indicies)
        b_input_positional_ids = smart_sort(b_input_positional_ids, indicies)
        b_labels = smart_sort(b_labels, indicies)

        b_prob_embeds = smart_sort(b_prob_embeds, indicies)

        logits, loss = model(
            b_input_ids,
            attention_mask=b_input_mask,
            positional_ids=b_input_positional_ids,
            seq_lengths=b_input_seq_lengths,
            sequence_length=MAX_LEN,
            labels=b_labels,
            position_embeds=b_prob_embeds
        )

        logits = F.softmax(logits, dim=1)

        # Accumulate the training loss over all of the batches so that we can
        # calculate the average loss at the end. `loss` is a Tensor containing a
        # single value; the `.item()` function just returns the Python value
        # from the tensor.
        total_train_loss.append(loss.item())

        # Perform a backward pass to calculate the gradients.
        loss.backward()

        # Clip the norm of the gradients to 1.0.
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # Update parameters and take a step using the computed gradient.
        # The optimizer dictates the "update rule"--how the parameters are
        # modified based on their gradients, the learning rate, etc.
        optimizer.step()

        # Update the learning rate.
        scheduler.step()

        logits_ = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        total_train_accuracy.append(flat_accuracy(logits_, label_ids))

        predictions_.append(np.argmax(logits_, axis=1))
        true_labels_.append(label_ids)

        # Progress update every 32 batches.
        if step % 16 == 0 and not step == 0:
            # Calculate elapsed time in minutes.
            elapsed = format_time(time.time() - t0)

            # Report progress.
            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:},     Loss: {:}.      Accuracy: {:}'.format(step, len(
                train_dataloader), elapsed, sum(total_train_loss[:step]) / step, sum(
                total_train_accuracy[:step]) / step))
            y_pred, y_true = [], []
            row = len(predictions_)
            for i in range(row):
                col = len(predictions_[i])
                for j in range(col):
                    tmp1 = predictions_[i][j]
                    tmp2 = true_labels_[i][j]
                    y_pred.append(label_list[int(tmp1)])
                    y_true.append(label_list[int(tmp2)])
            score(y_true, y_pred)

        # Report the final accuracy for this validation run.
    avg_train_accuracy = sum(total_train_accuracy) / len(train_dataloader)
    print("  Accuracy: {0:.2f}".format(avg_train_accuracy))

    # Calculate the average loss over all of the batches.
    avg_train_loss = sum(total_train_loss) / len(train_dataloader)

    # Measure how long this epoch took.
    training_time = format_time(time.time() - t0)

    print("")
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epcoh took: {:}".format(training_time))

    SAVE_EPOCH = epochs
    SAVE_PATH = "gpt_bert_bilstm_" + str(epoch_i) + ".pt"
    SAVE_LOSS = loss

    torch.save({
        'epoch': SAVE_EPOCH,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': SAVE_LOSS,
    }, SAVE_PATH)

    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    print("")
    print("Running Validation...")

    t0 = time.time()

    # Put the model in evaluation mode--the dropout layers behave differently
    # during evaluation.
    model.eval()

    # Tracking variables
    total_eval_accuracy = 0
    total_eval_loss = 0
    nb_eval_steps = 0
    predictions_, true_labels_ = [], []

    # Evaluate data for one epoch
    for batch in dev_dataloader:

        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_input_positional_ids = batch[2].to(device)
        b_input_seq_lengths = batch[3].to(device)
        b_labels = batch[4].to(device)

        b_prob_embeds = torch.zeros((len(b_input_ids), max_len, dim))
        with torch.no_grad():
            probs = decoder(b_input_ids, b_input_mask)
        b_prob_embeds = get_prob_embeds(probs, max_len)

        model.zero_grad()

        cur_len = len(b_input_ids)

        b_input_seq_lengths, indicies = torch.sort(b_input_seq_lengths, dim=0, descending=True)


        def smart_sort(x, per):
            z = torch.empty_like(x)
            for i in range(len(per)):
                z[per[i]] = x[i]
            return z


        b_input_ids = smart_sort(b_input_ids, indicies)
        b_input_mask = smart_sort(b_input_mask, indicies)
        b_input_positional_ids = smart_sort(b_input_positional_ids, indicies)
        b_labels = smart_sort(b_labels, indicies)

        b_prob_embeds = smart_sort(b_prob_embeds, indicies)

        with torch.no_grad():
            logits, loss = model(
                b_input_ids,
                attention_mask=b_input_mask,
                positional_ids=b_input_positional_ids,
                seq_lengths=b_input_seq_lengths,
                sequence_length=MAX_LEN,
                labels=b_labels,
                position_embeds=b_prob_embeds
            )

        logits = F.softmax(logits, dim=1)

        # Accumulate the validation loss.
        total_eval_loss += loss.item()

        # Move logits and labels to CPU
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        predictions_.append(np.argmax(logits, axis=1))
        true_labels_.append(label_ids)

        # Calculate the accuracy for this batch of test sentences, and
        # accumulate it over all batches.
        total_eval_accuracy += flat_accuracy(logits, label_ids)

    if epoch_i < 3:
        y_pred, y_true = [], []
        row = len(predictions_)
        for i in range(row):
            col = len(predictions_[i])
            for j in range(col):
                tmp1 = predictions_[i][j]
                tmp2 = true_labels_[i][j]
                y_pred.append(label_list[int(tmp1)])
                y_true.append(label_list[int(tmp2)])
        score(y_true, y_pred)
        df_result = pd.DataFrame(y_pred)
        df_result.to_csv('./dataset/dev_pred.csv', header=None, index=False)

        df_result = pd.DataFrame(y_true)
        df_result.to_csv('./dataset/dev_gt.csv', header=None, index=False)

    # Report the final accuracy for this validation run.
    avg_dev_accuracy = total_eval_accuracy / len(dev_dataloader)
    print("  Accuracy: {0:.2f}".format(avg_dev_accuracy))

    # Calculate the average loss over all of the batches.
    avg_dev_loss = total_eval_loss / len(dev_dataloader)

    # Measure how long the validation run took.
    dev_time = format_time(time.time() - t0)

    print("  Validation Loss: {0:.2f}".format(avg_dev_loss))
    print("  Validation took: {:}".format(dev_time))

    # Record all statistics from this epoch.
    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Valid. Loss': avg_dev_loss,
            'Valid. Accur.': avg_dev_accuracy,
            'Training Time': training_time,
            'Validation Time': dev_time
        }
    )

print("")
print("Training complete!")

print("Total training took {:} (h:mm:ss)".format(format_time(time.time() - total_t0)))

"""## **5. Prediction**

Predict test data
"""

#
# test_sentences = np.array(test_df.sentence.values)
# test_masked_sentences = np.array(test_df.masked_sentence.values)
# test_labels = np.array([label_list.index(label) for label in test_df.label.values])
#
# test_input_ids = torch.load('./dataset/test_input_ids.pt').to('cpu')
# test_attribute_masks = torch.load('./dataset/test_attribute_masks.pt').to('cpu')
# test_positional_ids = torch.load('./dataset/test_positional_ids.pt').to('cpu')
# test_seq_length = torch.load('./dataset/test_seq_length.pt').to('cpu')
# test_gt_labels = torch.load('./dataset/test_gt_labels.pt').to('cpu')
#
# max_len = 450
# batch_size = 3
# dim = 768
#
# test_prob_embeds = torch.zeros((len(test_input_ids), max_len, dim)).to('cpu')
# for i in range(len(test_input_ids)):
#     test_prob_embeds[i, :] = torch.load(.to('cpu')
#
# test_dataset = TensorDataset(test_input_ids, test_attribute_masks, test_positional_ids, test_seq_length, test_gt_labels,
#                              test_prob_embeds)
# test_sampler = SequentialSampler(test_dataset)
# test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)

# Put model in evaluation mode
# model.eval()
# # Tracking variables
# predictions_, true_labels_ = [], []
#
# # Predict
# for batch in test_dataloader:
#     # Add batch to GPU
#     batch = tuple(t.to(device) for t in batch)
#
#     # Unpack the inputs from our dataloader
#     b_input_ids, b_input_mask, b_input_positional_ids, b_input_seq_lengths, b_labels, b_prob_embeds = batch
#
#     model.zero_grad()
#
#     cur_len = len(b_input_ids)
#
#     b_input_seq_lengths, indicies = torch.sort(b_input_seq_lengths, dim=0, descending=True)
#
#     b_input_ids = smart_sort(b_input_ids, indicies)
#     b_input_mask = smart_sort(b_input_mask, indicies)
#     b_input_positional_ids = smart_sort(b_input_positional_ids, indicies)
#     b_labels = smart_sort(b_labels, indicies)
#     b_prob_embeds = smart_sort(b_prob_embeds, indicies)
#
#     # Telling the model not to compute or store gradients, saving memory and
#     # speeding up prediction
#     with torch.no_grad():
#         # Forward pass, calculate logit predictions
#         logits, loss = model(b_input_ids,
#                              attention_mask=b_input_mask,
#                              positional_ids=b_input_positional_ids,
#                              seq_lengths=b_input_seq_lengths,
#                              sequence_length=MAX_LEN,
#                              labels=b_labels,
#                              position_embeds=b_prob_embeds
#                              )
#         logits = F.softmax(logits, dim=1)
#     # Move logits and labels to CPU
#
#     logits = logits.detach().cpu().numpy()
#     label_ids = b_labels.to('cpu').numpy()
#     b_prob_embeds.to('cpu')
#
#     predictions_.append(np.argmax(logits, axis=1))
#     true_labels_.append(label_ids)
#
# y_pred, y_true = [], []
# row = len(predictions_)
# for i in range(row):
#     col = len(predictions_[i])
#     for j in range(col):
#         tmp1 = predictions_[i][j]
#         tmp2 = true_labels_[i][j]
#         y_pred.append(label_list[int(tmp1)])
#         y_true.append(label_list[int(tmp2)])
#
# df_result = pd.DataFrame(y_pred)
# df_result.to_csv('./dataset/test_pred.csv', header=None, index=False)
#
# df_result = pd.DataFrame(y_true)
# df_result.to_csv('./dataset/test_gt.csv', header=None, index=False)
