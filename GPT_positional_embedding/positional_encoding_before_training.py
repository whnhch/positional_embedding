# -*- coding: utf-8 -*-
"""Positional_Encoding_At_Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wDeaZFGaKu9i6Fy0rmJwvesdX3sg3huM

# **Implement Positional Encoding with GPT2 output**
----
"""

# #### **Install trsansformers** 

# package for using BERT from huggingface
# https://huggingface.co/transformers/index.html

# !pip install transformers

# for gpu
import torch
import math

if torch.cuda.is_available():
    device = torch.device('cuda')
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))

else:
    print('No GPU avbailable, using the CPU instead')
    device = torch.device('cpu')

import os

os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

"""## **1. Implement GPT2 Decoder**"""

import transformers
import torch
from transformers import GPT2Model
from PIL import Image
import numpy as np


class GPTDecoder(torch.nn.Module):
    def __init__(self, pretrained_model_name, max_len):
        super().__init__()

        self.decoder = GPT2Model.from_pretrained(pretrained_model_name)
        self.wte = self.decoder.get_input_embeddings().weight
        self.softmax = torch.nn.Softmax(dim=2)
        self.maxlen = max_len

    def get_decoder_output(self, inputs):
        outputs = self.decoder(inputs)
        last_hidden_state = outputs.last_hidden_state
        return last_hidden_state

    def get_logits(self, last_hidden_state):
        return torch.matmul(last_hidden_state, self.wte.T)  # (B,P,768)

    def get_probs(self, logits):
        return self.softmax(logits)

    def get_token_probs(self, input_ids, probs, attention_masks):
        output_prob = []
        # probs only of sequences
        # batch가 없다면
        for b in range(len(input_ids)):
            mask_idx = (attention_masks[b] == 0).nonzero()[0][0]

            seq_prob = []
            for i in range(self.maxlen):
                tok_prob = probs[b, i, input_ids[b, :]]  # (1, H)
                tok_prob[mask_idx:] = 0
                seq_prob.append(tok_prob)  # (i, H)
            output_prob.append(seq_prob)  # ( b, P', P')

        return output_prob

    def forward(self, input_ids, attention_masks):  # (B, P)
        last_hidden_state = self.get_decoder_output(input_ids)  # (B, P', H)
        logits = self.get_logits(last_hidden_state)  # (B, P', len(vocab))
        probs = self.get_probs(logits)  # (B, P', len(vocab))

        output_prob = self.get_token_probs(input_ids, probs, attention_masks)  # (B, P', P')

        return output_prob

    def get_words_probs(self, tokenizer, input_ids, probs):
        # convert id to token
        # without <|endoftext|>
        sub_texts = tokenizer.convert_ids_to_tokens(input_ids)
        words = []
        words_probs = []

        i = 0
        while i < len(sub_texts):
            sub = sub_texts[i]
            if '<|endoftext|>' in sub:
                break

            if '\u0120' in sub:
                cur_sub = []
                cur_sub.append(sub)

                for j in range(i + 1, len(sub_texts)):
                    if '\u0120' in sub_texts[j] or '<|endoftext|>' in sub_texts[j]:
                        j -= 1
                        break
                    else:
                        cur_sub.append(sub_texts[j])

                word = "".join(cur_sub)
                words.append(word.replace('\u0120', ''))
                words_probs.append(probs[j])
                i = j + 1

            else:
                cur_sub = []
                cur_sub.append(sub)

                for j in range(i + 1, len(sub_texts)):
                    if '\u0120' in sub_texts[j] or '<|endoftext|>' in sub_texts[j]:
                        j -= 1
                        break
                    else:
                        cur_sub.append(sub_texts[j])

                word = "".join(cur_sub)
                words.append(word.replace('\u0120', ''))
                words_probs.append(probs[j])
                i = j + 1

                # words.append(sub)
                # words_probs.append(probs[i])
                # i+=1

        return words, words_probs


"""### **GPT2 tokenizer utils**

"""

from transformers import GPT2Tokenizer


def get_gpt_tokenizer(gpt_model_name):
    return GPT2Tokenizer.from_pretrained(gpt_model_name)


def get_gpt_tokens(tokenizer, sentences, pretrained_model_name, max_len):
    input_ids = []
    attention_masks = []
    positional_ids = []

    return input_ids, attention_masks, positional_ids


def get_gpt_token_ids(tokenizer, sentences, max_len):
    input_ids = []
    attention_masks = []
    for sentence in sentences:
        outputs = tokenizer.encode_plus(sentence, add_special_tokens=True, pad_to_max_length=True,
                                        return_attention_mask=True, max_length=max_len,
                                        return_tensors='pt')
        input_ids.append(outputs['input_ids'])
        attention_masks.append(outputs['attention_mask'])

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)

    return input_ids, attention_masks


"""## **2. Data Preprocessing**

### **Tokenizer**

The sentences in our dataset obviously have varying lengths, so how does BERT handle this?

The tokenizer.encode_plus function combines multiple steps for us:


1.   Split the sentence into tokens.
2.   Add the special [CLS] and [SEP] tokens.
3.   Map the tokens to their IDs.
4.   Pad or truncate all sentences to the same length.
5.   Create the attention masks which explicitly differentiate real tokens from [PAD] tokens.

Set MAX_LEN to 512

### **Positional Representation** 
As denoted at the paper, positional rerpesentations are used at bidirectional LSTM. I implemented positional representation before feeding original sequences to BERT. Positional rerpesentation can be obtained and be injected to model with original token sequences using Dataloader. Furthermore, it is quite dull that change output of BERT to words and then get positional representation. Thus, get positoinal representation when tokenizing sequences.

<!-- Also, I added negative number at token after first [SEP]. The paper removed all tokens after first [SEP] at the output of BERT(contextual representation) this can be done by input_ids which included segement id. However, for easy application I just added negative length of sequence at token following first [SEP] of positional representation.  -->

Set length of positional representations as sequence legnth without pad
"""

from transformers import BertTokenizer, GPT2Tokenizer

print('Loading BERT tokenizer...')
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')

gpt_tokenizer.pad_token = gpt_tokenizer.eos_token

object_type_list = ['OBJORGANIZATION',
                    'OBJNATIONALITY',
                    'OBJMISC',
                    'OBJRELIGION',
                    'OBJNUMBER',
                    'OBJCRIMINALCHARGE',
                    'OBJIDEOLOGY',
                    'OBJSTATEORPROVINCE',
                    'OBJCITY',
                    'OBJCOUNTRY',
                    'OBJPERSON',
                    'OBJTITLE',
                    'OBJLOCATION',
                    'OBJCAUSEOFDEATH',
                    'OBJDURATION',
                    'OBJURL',
                    'OBJDATE']
subject_type_list = ['SUBJORGANIZATION', 'SUBJPERSON']

type_list = object_type_list + subject_type_list
bert_tokenizer.add_tokens(type_list, special_tokens=True)


def getSubjStart(sentence, s):
    if sentence[s] in subject_type_list:
        return s
    return -1


def getSubjEnd(sentence, s):
    se = s
    if sentence[s] in subject_type_list:
        se += 1
    return se


def getObjStart(sentence, o):
    if sentence[o] in object_type_list:
        return o
    return -1


def getObjEnd(sentence, o):
    oe = o
    if sentence[o] in object_type_list:
        oe += 1
    return oe


def isEnd(i, length):
    if i + 1 >= length:
        return True
    return False


def getPE(max_length, sentence):
    length = len(sentence)
    ps = [0] * (length)
    po = [0] * (length)

    ss, se, os, oe = 0, 0, 0, 0

    flagS = False
    flagO = False
    for i in range(length):
        if sentence[i] == 'PAD':
            break

        if not isEnd(i, length):
            tmps = getSubjStart(sentence, i)
            tmpo = getObjStart(sentence, i)

            if tmps != -1 and not flagS:
                ss = tmps
                flagS = True
            elif tmps != -1 and flagS:
                tmp = getSubjEnd(sentence, i)
                if tmp > ss:
                    se = tmp

            if tmpo != -1 and not flagO:
                os = tmpo
                flagO = True
            elif tmpo != -1 and flagO:
                tmp = getObjEnd(sentence, i)
                if tmp > os:
                    oe = tmp

    real_length = 0
    firstSEP = True
    for i in range(len(sentence)):
        if i != 0 and real_length == i:
            break
        if sentence[i] == '[SEP]' and firstSEP:
            firstSEP = False
            real_length = i
            break
        elif sentence[i] == '[SEP]' and not firstSEP:
            break

        elif sentence[i] == '[PAD]' or sentence[i] == '[UKN]':
            ps[i] = 0
            po[i] = 0
            continue

        # -127 ... 0 ... 127
        # 1 ...   128 ... 256
        if i < ss:
            ps[i] = abs(i - ss)
        elif ss <= i <= se:
            ps[i] = max_length
        elif i > se:
            ps[i] = (i - se) + max_length

        if i < os:
            po[i] = abs(i - os)
        elif os <= i <= oe:
            po[i] = max_length
        elif i > oe:
            po[i] = (i - oe) + max_length

        # if real_length == 0:
        #   real_length = max_length
    return [ps, po], real_length


def get_bilstm_positional_ids(max_length, word_sequences):
    return getPE(max_length, word_sequences)


def get_bert_token_probs(bert_input_ids, words, word_probs, max_len):
    tokens = bert_tokenizer.convert_ids_to_tokens(bert_input_ids)
    output = [torch.tensor([0] * max_len).to(device)] * len(tokens)
    i = 0
    k = i
    while i < len(words):

        token = tokens[k]
        if token == '[CLS]':
            k += 1
            continue
        if token == '[PAD]' or token == '[SEP]':
            break

        if not '##' in token:
            tmp_tokens = []
            tmp_tokens.append(token)

            j = k + 1
            while j < len(tokens):
                # print('tmp_tokens ', tmp_tokens)
                token = tokens[j]
                if not '##' in token:
                    j -= 1
                    break
                else:
                    tmp_tokens.append(token.replace('#', ''))
                    j += 1

            word = ''.join(tmp_tokens)
            if words[i] == word:
                for w in range(k, j + 1):
                    output[w] = word_probs[i]  # due to [cls] first token in bert words vs first token in gpt words
            else:
                try:
                    idx = type_list.index(word)
                    for w in range(k, j + 1):
                        output[w] = word_probs[i]
                except:
                    cur_word = words[i]
                    # print('cur word', cur_word)
                    bert_tokens = bert_tokenizer.convert_ids_to_tokens(
                        bert_tokenizer(cur_word, add_special_tokens=False)['input_ids'])
                    # print('cur bert tokens ', bert_tokens)
                    token_len = len(bert_tokens)
                    # print('cur s ', tokens[k:k+token_len])
                    if tokens[k:k + token_len] == bert_tokens:
                        for w in range(k, k + token_len):
                            output[w] = word_probs[i]
                        j = k + token_len - 1
                    else:
                        print('Error at get bert token probabilities! : No matching word in bert tokens. idx ', i,
                              ' word ', word, ' gpt ', bert_tokens)

            k = j + 1
            i += 1

        else:
            output[k] = word_probs[i]
            k += 1
            i += 1

    return torch.stack(output)


def get_bert_tokens(tokenizer, sentences, labels, max_len):
    input_ids = []
    attention_masks = []
    positional_ids = []
    sequence_lengths = []
    # pos_probs = []

    for i in range(len(sentences)):
        sent = sentences[i]
        encoded_dict = tokenizer.encode_plus(
            sent,  # Sentence to encode.
            add_special_tokens=False,  # Add '[CLS]' and '[SEP]'
            max_length=max_len,  # Pad & truncate all sentences.
            pad_to_max_length=True,
            return_attention_mask=True,  # Construct attn. masks.
            return_tensors='pt',  # Return pytorch tensors.
        )
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])

        tokens = tokenizer.convert_ids_to_tokens(encoded_dict['input_ids'][0, :])

        # pos_probs.append(get_bert_token_probs(tokens, words[i], words_probs[i],max_len))

        positional_id, sequence_length = get_bilstm_positional_ids(max_len, tokens)
        positional_ids.append(positional_id)
        sequence_lengths.append(sequence_length)

        # if i == 2:
        # break

    # must drop out after SEP of object and subject
    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    labels = torch.tensor(labels)

    positional_ids = torch.tensor(positional_ids)
    sequence_lengths = torch.tensor(sequence_lengths)
    # pos_probs = torch.stack(pos_probs)

    return input_ids, attention_masks, positional_ids, sequence_lengths, labels


"""### **Positional Encoding with GPT2 output** """


def get_one_hot_encoding(max_len, dim):
    pe = torch.zeros(max_len, dim)

    for i in range(max_len):
        pe[i, i] = 1

    return pe


def get_linear_embedding(max_len, dim):
    isOdd = False

    if dim % 2 != 0:
        isOdd = True

    pe = torch.zeros(max_len, dim)

    width_ratio = 1 / (int(dim / 2))
    height_ratio = 2 / (max_len - 1)

    for i in range(dim):
        pe[0, i] = 1 - (2 / (dim - 1)) * i

    for i in range(1, max_len):
        for j in range(int(dim / 2)):
            pe[i, j] = pe[i - 1, j] - (height_ratio - (height_ratio * width_ratio * j))
            pe[i, dim - 1 - j] = pe[i - 1, dim - 1 - j] + (height_ratio - (height_ratio * width_ratio * j))
        if isOdd:
            pe[i, int(dim / 2) + 1] = pe[i - 1, int(dim / 2) + 1] - (height_ratio - (height_ratio * width_ratio * j))

    return pe


def get_sinusoidal_embedding(max_len, dim=768):
    pe = torch.zeros(max_len, dim)

    for pos in range(max_len):
        for i in range(0, dim, 2):
            pe[pos, i] = \
                math.sin(pos / (10000 ** ((2 * i) / dim)))
            pe[pos, i + 1] = \
                math.cos(pos / (10000 ** ((2 * (i + 1)) / dim)))

    return pe


import pandas as pd
import numpy as np

label_list = ['org:member_of', 'per:schools_attended', 'per:charges', 'org:city_of_headquarters',
              'org:country_of_headquarters', 'org:subsidiaries', 'per:employee_of', 'per:stateorprovince_of_death',
              'per:stateorprovince_of_birth', 'per:country_of_death', 'org:shareholders', 'per:countries_of_residence',
              'per:children', 'org:alternate_names', 'per:alternate_names', 'per:stateorprovinces_of_residence',
              'per:country_of_birth', 'org:founded_by', 'org:parents', 'org:stateorprovince_of_headquarters',
              'org:dissolved', 'org:members', 'per:age', 'per:spouse', 'org:website', 'per:cities_of_residence',
              'per:parents', 'per:cause_of_death', 'per:date_of_death', 'per:origin', 'no_relation', 'per:religion',
              'org:political/religious_affiliation', 'per:siblings', 'org:founded', 'per:date_of_birth',
              'per:city_of_death', 'org:number_of_employees/members', 'org:top_members/employees', 'per:other_family',
              'per:title', 'per:city_of_birth'
              ]

train_df = pd.read_csv('./dataset/train_masked.tsv', delimiter='\t', header=None,
                       names=['sentence_id', 'label', 'label_notes', 'sentence', 'masked_sentence'])
dev_df = pd.read_csv('./dataset/dev_masked.tsv', delimiter='\t', header=None,
                     names=['sentence_id', 'label', 'label_notes', 'sentence', 'masked_sentence'])
test_df = pd.read_csv('./dataset/test_masked.tsv', delimiter='\t', header=None,
                      names=['sentence_id', 'label', 'label_notes', 'sentence', 'masked_sentence'])

train_sentences = np.array(train_df.sentence.values)
train_masked_sentences = np.array(train_df.masked_sentence.values)
train_labels = np.array([label_list.index(label) for label in train_df.label.values])

dev_sentences = np.array(dev_df.sentence.values)
dev_masked_sentences = np.array(dev_df.masked_sentence.values)
dev_labels = np.array([label_list.index(label) for label in dev_df.label.values])

test_sentences = np.array(test_df.sentence.values)
test_masked_sentences = np.array(test_df.masked_sentence.values)
test_labels = np.array([label_list.index(label) for label in test_df.label.values])
"""### **Make torch dataloader**

Minimum of train sequence length should not be zero it means that there is any sequeence which size is over MAX_LEN
"""

batch_size = 3
max_len = 450

import pickle

with open('test_words.txt', 'rb') as f1, open('test_probs.txt', 'rb') as f2:
    test_words = pickle.load(f1)
    test_probs = pickle.load(f2)

with open('dev_words.txt', 'rb') as f1, open('dev_probs.txt', 'rb') as f2:
    dev_words = pickle.load(f1)
    dev_probs = pickle.load(f2)

with open('train_words.txt', 'rb') as f1, open('train_probs.txt', 'rb') as f2:
    train_words = pickle.load(f1)
    train_probs = pickle.load(f2)


def get_prob_embeds(bert_input_ids, words, probs, max_len):
    prob_embds = []
    for i in range(len(words)):
        bert_token_probs = get_bert_token_probs(bert_input_ids[i], words[i], probs[i], max_len)
        embds = get_linear_embedding(max_len)

        prob_embds.append(embds)

    torch.tensor(prob_embds)

    return prob_embds


import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

batch_size = 3
max_len = 450

tr_input_ids, tr_attribute_masks, tr_positional_ids, tr_seq_length, tr_gt_labels = get_bert_tokens(bert_tokenizer,
                                                                                                   train_masked_sentences,
                                                                                                   train_labels,
                                                                                                   max_len)

train_prob_embeds = get_prob_embeds(tr_input_ids, train_words, train_probs)
# print(torch.min(tr_seq_length)) # should not be zero it means that there is any sequeence which size is over MAX_LEN

train_dataset = TensorDataset(tr_input_ids, tr_attribute_masks, tr_positional_ids, tr_seq_length, tr_gt_labels,
                              train_prob_embeds)
train_sampler = RandomSampler(train_dataset)
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)

dv_input_ids, dv_attribute_masks, dv_positional_ids, dv_seq_length, dv_gt_labels = get_bert_tokens(bert_tokenizer,
                                                                                                   dev_masked_sentences,
                                                                                                   dev_labels, max_len)

dev_prob_embeds = get_prob_embeds(dv_input_ids, dev_words, dev_probs)
dev_dataset = TensorDataset(dv_input_ids, dv_attribute_masks, dv_positional_ids, dv_seq_length, dv_gt_labels,
                            dev_prob_embeds)
dev_sampler = SequentialSampler(dev_dataset)
dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=batch_size)

# B batch size, deafult = 32
# P length of sequence, default = 450
# H hidden size of BERT, default = 768
# E embedding dimension of embedding layer, default = 20

from transformers import BertModel, AdamW, BertConfig
import torch.nn as nn
from torch.autograd import Variable
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence


class BERT_BiLSTM(nn.Module):
    def __init__(self, tokenizer, max_length=450, num_labels=42, embedding_dim=20, num_recurrent_layers=1,
                 bidirectional=True, lstm_hidden_size=768, mlp_hidden_size=300) -> None:
        # super(BERT_BiLSTM, self).__init__()
        # Feed at [CLS] token

        super().__init__()
        self.tokenizer = tokenizer
        self.config = BertConfig.from_pretrained('bert-base-cased')
        self.config.output_hidden_states = True
        self.config.output_attentions = True

        self.bert = BertModel.from_pretrained("bert-base-cased",
                                              config=self.config
                                              # , add_pooling_layer=False
                                              )
        self.bert.resize_token_embeddings(len(tokenizer))
        self.dropout = nn.Dropout(0.1)
        self.num_labels = num_labels
        self.embedding_dim = embedding_dim
        self.embedding_layer = nn.Embedding(num_embeddings=max_length * 2,
                                            embedding_dim=embedding_dim,
                                            padding_idx=0
                                            )

        self.num_recurrent_layers = num_recurrent_layers
        self.bidirectional = bidirectional
        self.lstm_hidden_size = lstm_hidden_size

        self.lstm = nn.LSTM(self.lstm_hidden_size + self.embedding_dim + self.embedding_dim,
                            hidden_size=self.lstm_hidden_size,
                            batch_first=True,
                            bidirectional=True
                            )

        self.mlp = nn.Linear(in_features=self.lstm_hidden_size * 2, out_features=mlp_hidden_size)
        self.classifier = nn.Linear(in_features=mlp_hidden_size, out_features=self.num_labels)

    def forward(self, input_ids, attention_mask, positional_ids, seq_lengths, sequence_length, labels=None,
                position_embeds=None):
        # outputs: (last_encoder_layer, pooled_output, attention_weight)
        # Set add_pooling_layer
        bert_outputs = self.bert(input_ids=input_ids,
                                 attention_mask=attention_mask,
                                 encoder_hidden_states=torch.FloatTensor(
                                     (input_ids.shape[0], sequence_length, sequence_length)),
                                 position_embeds=position_embeds
                                 )

        sequence_output = bert_outputs[0]  # (B, P, H)

        for i in range(len(input_ids)):
            sequence_output[i, seq_lengths[i]:] = 0

        # LSTM
        ps = positional_ids[:, 0, :]
        po = positional_ids[:, 1, :]

        ps = self.embedding_layer(ps)
        po = self.embedding_layer(po)

        lstm_input = torch.cat((sequence_output, ps, po), dim=-1)  # (B, P, H+E+E)

        packed_lstm_input = pack_padded_sequence(lstm_input, seq_lengths.tolist(), batch_first=True,
                                                 enforce_sorted=False)

        h0 = torch.zeros(self.num_recurrent_layers * 2, input_ids.shape[0], self.lstm_hidden_size).to(device)
        c0 = torch.zeros(self.num_recurrent_layers * 2, input_ids.shape[0], self.lstm_hidden_size).to(device)

        packed_lstm_output, (lstm_hidden, lstm_cell) = self.lstm(packed_lstm_input, (h0, c0))  # (B, P, H*2) ...

        # mlp
        mlp_input = torch.cat((lstm_hidden[-2, :, :], lstm_hidden[-1, :, :]), dim=1)
        mlp_output = self.mlp(mlp_input)  # (B, H*2)

        # last layer
        # calcuate logits
        logits = self.classifier(mlp_output)

        # calcuate loss
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

        return logits, loss


"""### **Generate model**"""

max_len = 450
model = BERT_BiLSTM(bert_tokenizer, max_length=max_len)
model.to(device)

"""## **3. Optimizer & Learning Rate Scheduler**

For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the **BERT paper**):

* Batch size: 8, 16, 32, 64, 128
* Learning rate (Adam): 3e-4, 1e-4, 5e-5, 3e-5
* Number of epochs: 2, 3, 4

For papaer,

* Learning rate : 5e-5
  * which is not workiing properly
  
Set BATCH_SIZE = 3, LEARNING_RATE=1e-5, EPOCHS=3
"""

batch_size = 3  # (set when creating our DataLoaders)
learning_rate = 1e-5
epochs = 3

optimizer = AdamW(model.parameters(),
                  lr=learning_rate,  # args.learning_rate - default is 5e-5, our notebook had 2e-5
                  eps=1e-12  # args.adam_epsilon  - default is 1e-8.
                  )

from transformers import get_linear_schedule_with_warmup

total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=0,  # Default value in run_glue.py
                                            num_training_steps=total_steps)

"""## **4. Training Loop**

**Training**
* Unpack our data inputs and labels
* Load data onto the GPU for accerlation
* Clear out the gradients calculated the previous pass
* Forward pass
* Backward pass
* Tell the network to update parameteres with optimizer.step()
* Track variables for monitoring progress

**Evaluation**
* Unpack our data inputs and labeld
* Load data onto the GPU for acceleration
* Forward pass
* Compute loss on our validation data and track variables for monitoring progress
"""

import numpy as np
import torch.nn.functional as F


# Function to calculate the accuracy of our predictions vs labels
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)


import time
import datetime


def format_time(elapsed):
    '''
    Takes a time in seconds and returns a string hh:mm:ss
    '''
    # Round to the nearest second.
    elapsed_rounded = int(round((elapsed)))

    # Format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))


import random
import numpy as np
import torch.nn.functional as F

seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

decoder = GPTDecoder('gpt2', max_len)
decoder.to(device)

decoder.eval()
training_stats = []

total_t0 = time.time()

MAX_LEN = 450

# For each epoch...
for epoch_i in range(epochs):

    # ========================================
    #               Training
    # ========================================

    # Perform one full pass over the training set.

    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    print('Training...')

    # Measure how long the training epoch takes.
    t0 = time.time()

    # Reset the total loss for this epoch.
    total_train_loss = []
    total_train_accuracy = []

    model.train()

    # For each batch of training data...
    for step, batch in enumerate(train_dataloader):

        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_input_positional_ids = batch[2].to(device)
        b_input_seq_lengths = batch[3].to(device)
        b_labels = batch[4].to(device)
        b_prob_embeds = batch[5].to(device)

        model.zero_grad()

        cur_len = len(b_input_ids)

        b_input_seq_lengths, indicies = torch.sort(b_input_seq_lengths, dim=0, descending=True)


        def smart_sort(x, per):
            z = torch.empty_like(x)
            for i in range(len(per)):
                z[per[i]] = x[i]
            return z


        b_input_ids = smart_sort(b_input_ids, indicies)
        b_input_mask = smart_sort(b_input_mask, indicies)
        b_input_positional_ids = smart_sort(b_input_positional_ids, indicies)
        b_labels = smart_sort(b_labels, indicies)

        prob_embeds = smart_sort(b_prob_embeds, indicies)

        logits, loss = model(
            b_input_ids,
            attention_mask=b_input_mask,
            positional_ids=b_input_positional_ids,
            seq_lengths=b_input_seq_lengths,
            sequence_length=MAX_LEN,
            labels=b_labels,
            position_embeds=prob_embeds
        )

        logits = F.softmax(logits, dim=1)

        # Accumulate the training loss over all of the batches so that we can
        # calculate the average loss at the end. `loss` is a Tensor containing a
        # single value; the `.item()` function just returns the Python value 
        # from the tensor.
        total_train_loss.append(loss.item())

        # Perform a backward pass to calculate the gradients.
        loss.backward()

        # Clip the norm of the gradients to 1.0.
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # Update parameters and take a step using the computed gradient.
        # The optimizer dictates the "update rule"--how the parameters are
        # modified based on their gradients, the learning rate, etc.
        optimizer.step()

        # Update the learning rate.
        scheduler.step()

        logits_ = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        total_train_accuracy.append(flat_accuracy(logits_, label_ids))

        # Progress update every 32 batches.
        if step % 32 == 0 and not step == 0:
            # Calculate elapsed time in minutes.
            elapsed = format_time(time.time() - t0)

            # Report progress.
            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:},     Loss: {:}.      Accuracy: {:}'.format(step, len(
                train_dataloader), elapsed, sum(total_train_loss[:step]) / step, sum(
                total_train_accuracy[:step]) / step))

        # Report the final accuracy for this validation run.
    avg_train_accuracy = sum(total_train_accuracy) / len(train_dataloader)
    print("  Accuracy: {0:.2f}".format(avg_train_accuracy))

    # Calculate the average loss over all of the batches.
    avg_train_loss = sum(total_train_loss) / len(train_dataloader)

    # Measure how long this epoch took.
    training_time = format_time(time.time() - t0)

    print("")
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epcoh took: {:}".format(training_time))

    SAVE_EPOCH = epochs
    SAVE_PATH = "model_max_len_450_whcho_" + str(epoch_i) + ".pt"
    SAVE_LOSS = loss

    torch.save({
        'epoch': SAVE_EPOCH,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': SAVE_LOSS,
    }, SAVE_PATH)
    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    print("")
    print("Running Validation...")

    t0 = time.time()

    # Put the model in evaluation mode--the dropout layers behave differently
    # during evaluation.
    model.eval()

    # Tracking variables 
    total_eval_accuracy = 0
    total_eval_loss = 0
    nb_eval_steps = 0
    predictions_, true_labels_ = [], []

    # Evaluate data for one epoch
    for batch in dev_dataloader:

        # Unpack this training batch from our dataloader. 
        #
        # As we unpack the batch, we'll also copy each tensor to the GPU using 
        # the `to` method.
        #
        # `batch` contains three pytorch tensors:
        #   [0]: input ids 
        #   [1]: attention masks
        #   [2]: labels 
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_input_positional_ids = batch[2].to(device)
        b_input_seq_lengths = batch[3].to(device)
        b_labels = batch[4].to(device)
        b_prob_embeds = batch[5].to(device)

        model.zero_grad()

        cur_len = len(b_input_ids)

        b_input_seq_lengths, indicies = torch.sort(b_input_seq_lengths, dim=0, descending=True)


        def smart_sort(x, per):
            z = torch.empty_like(x)
            for i in range(len(per)):
                z[per[i]] = x[i]
            return z


        b_input_ids = smart_sort(b_input_ids, indicies)
        b_input_mask = smart_sort(b_input_mask, indicies)
        b_input_positional_ids = smart_sort(b_input_positional_ids, indicies)
        b_labels = smart_sort(b_labels, indicies)

        prob_embeds = smart_sort(b_prob_embeds, indicies)

        logits, loss = model(
            b_input_ids,
            attention_mask=b_input_mask,
            positional_ids=b_input_positional_ids,
            seq_lengths=b_input_seq_lengths,
            sequence_length=MAX_LEN,
            labels=b_labels,
            position_embeds=prob_embeds
        )

        logits = F.softmax(logits, dim=1)

        # Accumulate the validation loss.
        total_eval_loss += loss.item()

        # Move logits and labels to CPU
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        predictions_.append(np.argmax(logits, axis=1))
        true_labels_.append(label_ids)

        # Calculate the accuracy for this batch of test sentences, and
        # accumulate it over all batches.
        total_eval_accuracy += flat_accuracy(logits, label_ids)

    if epoch_i < 3:
        y_pred, y_true = [], []
        row = len(predictions_)
        for i in range(row):
            col = len(predictions_[i])
            for j in range(col):
                tmp1 = predictions_[i][j]
                tmp2 = true_labels_[i][j]
                y_pred.append(label_list[int(tmp1)])
                y_true.append(label_list[int(tmp2)])

        df_result = pd.DataFrame(y_pred)
        df_result.to_csv('./dataset/dev_pred.csv', header=None, index=False)

        df_result = pd.DataFrame(y_true)
        df_result.to_csv('./dataset/dev_gt.csv', header=None, index=False)

    # Report the final accuracy for this validation run.
    avg_dev_accuracy = total_eval_accuracy / len(dev_dataloader)
    print("  Accuracy: {0:.2f}".format(avg_dev_accuracy))

    # Calculate the average loss over all of the batches.
    avg_dev_loss = total_eval_loss / len(dev_dataloader)

    # Measure how long the validation run took.
    dev_time = format_time(time.time() - t0)

    print("  Validation Loss: {0:.2f}".format(avg_dev_loss))
    print("  Validation took: {:}".format(dev_time))

    # Record all statistics from this epoch.
    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Valid. Loss': avg_dev_loss,
            'Valid. Accur.': avg_dev_accuracy,
            'Training Time': training_time,
            'Validation Time': dev_time
        }
    )

print("")
print("Training complete!")

print("Total training took {:} (h:mm:ss)".format(format_time(time.time() - total_t0)))

"""## **5. Prediction**

Predict test data
"""

max_len = 450
batch_size = 3

test_input_ids, test_attribute_masks, test_positional_ids, test_seq_length, test_gt_labels = get_bert_tokens(
    bert_tokenizer,
    test_masked_sentences, test_labels,
    max_len)

test_prob_embeds = get_prob_embeds(test_input_ids, test_words, test_probs, max_len)
test_dataset = TensorDataset(test_input_ids, test_attribute_masks, test_positional_ids, test_seq_length, test_gt_labels,
                             test_prob_embeds)
test_sampler = SequentialSampler(test_dataset)
test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)

# Put model in evaluation mode
model.eval()
# Tracking variables 
predictions_, true_labels_ = [], []

# Predict 
for batch in test_dataloader:
    # Add batch to GPU
    batch = tuple(t.to(device) for t in batch)

    # Unpack the inputs from our dataloader
    b_input_ids, b_input_mask, b_input_positional_ids, b_input_seq_lengths, b_labels, b_prob_embeds = batch

    model.zero_grad()

    cur_len = len(b_input_ids)

    b_input_seq_lengths, indicies = torch.sort(b_input_seq_lengths, dim=0, descending=True)

    b_input_ids = smart_sort(b_input_ids, indicies)
    b_input_mask = smart_sort(b_input_mask, indicies)
    b_input_positional_ids = smart_sort(b_input_positional_ids, indicies)
    b_labels = smart_sort(b_labels, indicies)
    prob_embeds = smart_sort(b_prob_embeds, indicies)

    # Telling the model not to compute or store gradients, saving memory and
    # speeding up prediction
    with torch.no_grad():
        # Forward pass, calculate logit predictions
        logits, loss = model(b_input_ids,
                             attention_mask=b_input_mask,
                             positional_ids=b_input_positional_ids,
                             seq_lengths=b_input_seq_lengths,
                             sequence_length=MAX_LEN,
                             labels=b_labels,
                             position_embeds=prob_embeds
                             )
        logits = F.softmax(logits, dim=1)
    # Move logits and labels to CPU

    logits = logits.detach().cpu().numpy()
    label_ids = b_labels.to('cpu').numpy()

    predictions_.append(np.argmax(logits, axis=1))
    true_labels_.append(label_ids)

y_pred, y_true = [], []
row = len(predictions_)
for i in range(row):
    col = len(predictions_[i])
    for j in range(col):
        tmp1 = predictions_[i][j]
        tmp2 = true_labels_[i][j]
        y_pred.append(label_list[int(tmp1)])
        y_true.append(label_list[int(tmp2)])

df_result = pd.DataFrame(y_pred)
df_result.to_csv('./dataset/test_pred.csv', header=None, index=False)

df_result = pd.DataFrame(y_true)
df_result.to_csv('./dataset/test_gt.csv', header=None, index=False)
